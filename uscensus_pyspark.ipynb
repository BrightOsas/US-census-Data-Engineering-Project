{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26730729-024e-4c65-82d9-c8d917002f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03995bb0-252a-4a78-81f2-28a7def3a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .appName(\"PySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e16abd7-9bd8-4ab9-9674-ad82f966be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('finaldata.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4245be-f91b-471c-989e-6dcf32338de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\n",
    "    'DATE',\n",
    "    expr(\"concat_ws('/', HRYEAR4, lpad(HRMONTH, 2, '0'))\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5335d4b9-a31f-4166-ab07-2a920331054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.select(\n",
    "    col('HRHHID'),\n",
    "    col('DATE'),\n",
    "    col('HUFINAL'),\n",
    "    col('HEHOUSUT'),\n",
    "    col('HRHTYPE'),\n",
    "    col('HETELHHD'),\n",
    "    col('HETELAVL'),\n",
    "    col('HEPHONEO'),\n",
    "    col('HUINTTYP'),\n",
    "    col('HEFAMINC'),\n",
    "    col('GEDIV'),\n",
    "    col('PTDTRACE')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013b877c-4564-4066-be20-d3b1a6887270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+--------+-------+--------+--------+--------+--------+--------+-----+--------+\n",
      "|      HRHHID|   DATE|HUFINAL|HEHOUSUT|HRHTYPE|HETELHHD|HETELAVL|HEPHONEO|HUINTTYP|HEFAMINC|GEDIV|PTDTRACE|\n",
      "+------------+-------+-------+--------+-------+--------+--------+--------+--------+--------+-----+--------+\n",
      "|  4795110719|2017/12|    201|       1|      1|       1|      -1|       1|       2|       9|    6|       1|\n",
      "|  4795110719|2017/12|    201|       1|      1|       1|      -1|       1|       2|       9|    6|       1|\n",
      "| 71691004941|2017/12|    201|       1|      1|       1|      -1|       1|       1|      11|    6|       1|\n",
      "| 71691004941|2017/12|    201|       1|      1|       1|      -1|       1|       1|      11|    6|       1|\n",
      "| 71691004941|2017/12|    201|       1|      1|       1|      -1|       1|       1|      11|    6|       1|\n",
      "|110177987986|2017/12|    201|       1|      1|       1|      -1|       1|       1|      14|    6|       2|\n",
      "|110177987986|2017/12|    201|       1|      1|       1|      -1|       1|       1|      14|    6|       2|\n",
      "|110206593381|2017/12|    213|       1|      0|      -1|      -1|       0|       1|      -1|    6|      -1|\n",
      "|110284815680|2017/12|    201|       1|      7|       1|      -1|       1|       1|       9|    6|       1|\n",
      "|110327856469|2017/12|    201|       1|      7|       1|      -1|       1|       1|       5|    6|       2|\n",
      "+------------+-------+-------+--------+-------+--------+--------+--------+--------+--------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486b98c9-9db7-40dc-98fc-2e61bed09952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|HEFAMINC|Count of Responders|\n",
      "+--------+-------------------+\n",
      "|      -1|              20391|\n",
      "|      12|               9971|\n",
      "|       1|               3136|\n",
      "|      13|              13442|\n",
      "|       6|               4518|\n",
      "|      16|              15704|\n",
      "|       3|               2277|\n",
      "|       5|               2614|\n",
      "|      15|              17794|\n",
      "|       9|               6743|\n",
      "|       4|               3161|\n",
      "|       8|               5803|\n",
      "|       7|               6312|\n",
      "|      10|               6620|\n",
      "|      11|               9788|\n",
      "|      14|              16557|\n",
      "|       2|               1625|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"HEFAMINC\").count().withColumnRenamed(\"count\", \"Count of Responders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a80fedd-268a-4b48-92b5-d3dfe7251205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------+\n",
      "|GEDIV|PTDTRACE|HRHHID_count|\n",
      "+-----+--------+------------+\n",
      "|    5|       1|       16999|\n",
      "|    8|       1|       14343|\n",
      "|    9|       1|       13214|\n",
      "|    3|       1|       11325|\n",
      "|    7|       1|       11248|\n",
      "|    4|       1|        9884|\n",
      "|    2|       1|        8487|\n",
      "|    1|       1|        8410|\n",
      "|    6|       1|        6580|\n",
      "|    5|       2|        4899|\n",
      "+-----+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by 'GEDIV' and 'PTDTRACE', aggregate count, and sort in descending order\n",
    "question2 =df2.groupBy(\"GEDIV\", \"PTDTRACE\").agg({\"HRHHID\": \"count\"}).withColumnRenamed(\"count(HRHHID)\", \"HRHHID_count\").orderBy(\"HRHHID_count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48351169-3d83-4b7c-8710-ea4487cde461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.filter((col('HETELHHD') == 2) & (col('HETELAVL') == 1) & (col('HEPHONEO') == 1)).select(\"HRHHID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f44e78-bbd3-4f7c-9ef2-c3f70a3e22c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.filter((col('HETELHHD') == 1) & (col('HEPHONEO') == 2)).select(\"HRHHID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703da809-7a01-40f2-bae2-6384bacf4549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://94911c0926e4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Assessment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f987d25c4d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6f5b265-4e85-40ab-be90-856ef3ede965",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "120347f8-eb0c-4498-b94d-47fb2d27eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "microdata_df = spark.read.text(\"dec17pub.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebbe5790-c37d-4e30-9254-b0685dcdf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|00000479511071912...|\n",
      "|00000479511071912...|\n",
      "|00007169100494112...|\n",
      "|00007169100494112...|\n",
      "|00007169100494112...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "microdata_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9440cbcb-7f11-43eb-a7b9-799c351fc604",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = [\n",
    "    ('HRHHID', 0, 15),\n",
    "     ('HRMONTH', 15, 17),\n",
    "     ('HRYEAR4', 17, 21),\n",
    "     ('HUFINAL', 23, 26),\n",
    "     ('HEHOUSUT', 30, 32),\n",
    "     ('HETELHHD', 32, 34),\n",
    "     ('HETELAVL', 34, 36),\n",
    "     ('HEPHONEO', 36, 38),\n",
    "     ('HEFAMINC', 38, 40),\n",
    "     ('HRHTYPE', 60, 62),\n",
    "     ('HUINTTYP', 64, 66),\n",
    "     ('GEDIV', 90, 91),\n",
    "     ('PTDTRACE', 138, 140)   # Add more fields as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "135de521-cf2d-4b97-b152-c2f223d6ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StructType([StructField(field, StringType(), True) for field, _, _ in data_dictionary]))\n",
    "def extract_fields(line):\n",
    "    fields = {}\n",
    "    for field, start, end in data_dictionary:\n",
    "        fields[field] = line[start - 1:end]  # Adjust positions as Spark uses 1-based indexing\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22f9c371-d075-4b2e-9c40-4eab3865eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF using withColumn\n",
    "extracted_fields_df = microdata_df.withColumn(\"extracted_fields\", extract_fields(col(\"value\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb0a04-1747-4cfa-b63a-ce522a2873d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MicrodataAnalysis\").getOrCreate()\n",
    "\n",
    "# Load microdata into a DataFrame\n",
    "microdata_df = spark.read.text(\"microdata_file.dat\")\n",
    "\n",
    "# Assuming data dictionary is a list of tuples with (field_name, start_position, end_position)\n",
    "data_dictionary = [\n",
    "    (\"field1\", 1, 10),\n",
    "    (\"field2\", 16, 25),\n",
    "    # Add more fields as needed\n",
    "]\n",
    "\n",
    "# Define a UDF to extract fields based on positions\n",
    "@udf(StructType([StructField(field, StringType(), True) for field, _, _ in data_dictionary]))\n",
    "def extract_fields(line):\n",
    "    fields = {}\n",
    "    for field, start, end in data_dictionary:\n",
    "        fields[field] = line[start - 1:end]  # Adjust positions as Spark uses 1-based indexing\n",
    "    return fields\n",
    "\n",
    "# Apply the UDF using withColumn\n",
    "extracted_fields_df = microdata_df.withColumn(\"extracted_fields\", extract_fields(col(\"value\")))\n",
    "\n",
    "# Select relevant columns for further analysis\n",
    "selected_fields_df = extracted_fields_df.select(\"extracted_fields.field1\", \"extracted_fields.field2\")\n",
    "\n",
    "# Show the result\n",
    "selected_fields_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db98ffc8-cb85-48dc-b162-6205bc404390",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `field1` in `HRHHID`, `HRMONTH`, `HRYEAR4`, `HUFINAL`, `HEHOUSUT`, `HETELHHD`, `HETELAVL`, `HEPHONEO`, `HEFAMINC`, `HRHTYPE`, `HUINTTYP`, `GEDIV`, `PTDTRACE`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select relevant columns for further analysis\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m selected_fields_df \u001b[38;5;241m=\u001b[39m \u001b[43mextracted_fields_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_fields.field1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_fields.field2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [FIELD_NOT_FOUND] No such struct field `field1` in `HRHHID`, `HRMONTH`, `HRYEAR4`, `HUFINAL`, `HEHOUSUT`, `HETELHHD`, `HETELAVL`, `HEPHONEO`, `HEFAMINC`, `HRHTYPE`, `HUINTTYP`, `GEDIV`, `PTDTRACE`."
     ]
    }
   ],
   "source": [
    "# Select relevant columns for further analysis\n",
    "selected_fields_df = extracted_fields_df.select(\"extracted_fields.field1\", \"extracted_fields.field2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68758ef1-e51e-4c82-9590-4003cdec68cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a5b8b-1cbf-405f-a633-d3ab602bf85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
